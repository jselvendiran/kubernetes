
## Install kubeadm, Kubelet, kubectl

### References
 - https://kubernetes.io/docs/setup/independent/install-kubeadm/
 - https://kubernetes.io/docs/setup/independent/create-cluster-kubeadm/
 - https://kubernetes.io/docs/reference/setup-tools/kubeadm/kubeadm-init/
 - https://kubernetes.io/docs/reference/setup-tools/kubeadm/kubeadm-join/
 - https://gist.github.com/kevashcraft/5aa85f44634c37a9ee05dde7e83ac7e2
 - https://kubernetes.io/docs/setup/independent/create-cluster-kubeadm/
 - https://www.aquasec.com/wiki/display/containers/70+Best+Kubernetes+Tutorials
 - https://docs.projectcalico.org/v3.6/getting-started/kubernetes/
 - https://kubernetes.io/docs/concepts/cluster-administration/addons/#networking-and-network-policy
 - https://github.com/ramitsurana/awesome-kubernetes

## Make sure to always use the latest stable version on all kubernetes and its' addons


### Disable swap mode and get into root mode.
```shell
swapoff -a
sudo su
```

### Install CNI plugins (Container Network Interface required for most pod network):
```shell
CNI_VERSION="v0.6.0"
mkdir -p /opt/cni/bin
curl -L "https://github.com/containernetworking/plugins/releases/download/${CNI_VERSION}/cni-plugins-amd64-${CNI_VERSION}.tgz" | tar -C /opt/cni/bin -xz
```

### Install crictl (Container Runtime Interface (CRI) required for kubeadm / Kubelet)
```shell
CRICTL_VERSION="v1.11.1"
mkdir -p /opt/bin
curl -L "https://github.com/kubernetes-incubator/cri-tools/releases/download/${CRICTL_VERSION}/crictl-${CRICTL_VERSION}-linux-amd64.tar.gz" | tar -C /opt/bin -xz
```

### Install kubeadm, kubelet, kubectl:
 - kubeadm: the command to bootstrap the cluster (kube init, kube join).
 - kubelet: the primary “node agent” that runs on each node and does things like starting pods and containers.
 - kubectl: the command line util to talk to your cluster (inside or outside).

```shell
RELEASE="$(curl -sSL https://dl.k8s.io/release/stable.txt)"
mkdir -p /opt/bin
cd /opt/bin
curl -L --remote-name-all https://storage.googleapis.com/kubernetes-release/release/${RELEASE}/bin/linux/amd64/{kubeadm,kubelet,kubectl}
chmod +x {kubeadm,kubelet,kubectl}
```

### Add a kubelet systemd service
```shell
curl -sSL "https://raw.githubusercontent.com/kubernetes/kubernetes/${RELEASE}/build/debs/kubelet.service" | sed "s:/usr/bin:/opt/bin:g" > /etc/systemd/system/kubelet.service
mkdir -p /etc/systemd/system/kubelet.service.d
curl -sSL "https://raw.githubusercontent.com/kubernetes/kubernetes/${RELEASE}/build/debs/10-kubeadm.conf" | sed "s:/usr/bin:/opt/bin:g" > /etc/systemd/system/kubelet.service.d/10-kubeadm.conf
```

### exit from super user mode, Enable and start kubelet: 
```shell
exit
sudo systemctl enable --now kubelet
```

### Restarting the kubelet if required:
```shell
sudo systemctl daemon-reload
sudo systemctl restart kubelet
```

### Prior to kubeadm init - Enable docker service & verify connectivity to gcr.io registries:
```shell
sudo systemctl enable docker.service
sudo kubeadm config images pull
```

### Ignore cgroupfs driver warning message as cgroupfs is the docker default driver.
It is important to make sure the pod network is a non-existing network. This means that it should not be routable from any of your nodes and it may not be in the same range as your node interfaces. For example if your master node would have the IP: 192.168.0.1 and you assign the 192.168.0.0/16 range to your pod network this will cause issues. So in this case you should pick another network such as: /opt/bin/kubeadm init --apiserver-advertise-address=192.168.0.1 --pod-network-cidr=192.168.1.0/16

### Init the cluster (MasterNode)
```shell
priv_ip=$(ip -f inet -o addr show eth0|cut -d\  -f 7 | cut -d/ -f 1 | head -n 1)
sudo kubeadm init --apiserver-advertise-address=$priv_ip  --pod-network-cidr=10.244.0.0/16
```

### For kubectl to work in MasterNode
 - It is needed to join other worker node to the cluster
 - Copy the console output and save it.
```shell
mkdir -p $HOME/.kube
sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
sudo chown $(id -u):$(id -g) $HOME/.kube/config
```

### For kubectl to work in WorkerNode:
```shell
mkdir -p $HOME/.kube
sudo vi $HOME/.kube/config
copy the admin.config content from masternode and paste in the vi editor. save and exit vi editor.
```

### Installing a pod network add-on (Master & then all worker nodes)
```shell
kubectl apply -f https://docs.projectcalico.org/v3.6/getting-started/kubernetes/installation/hosted/kubernetes-datastore/calico-networking/1.7/calico.yaml
```
Or
```shell
kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/v0.11.0/Documentation/kube-flannel.yml
```

### Join the cluster (WorkerNode)
Sample token and hashkey:
```shell
kubeadm join 192.168.1.101:6443 --token i5f4a6.shvz07nd1a1h0yli --discovery-token-ca-cert-hash sha256:62f980861d949412076c95e222262e426566db87bd3e8c2aa63995ad616df2cb
```
If Token & Hash expired:
```shell
kubeadm token create --print-join-command
```

### To interact with kubernetes cluster from outside cluster
 - Install kubectl and set in path
```shell
scp -i ~/.kube/<privatekeyfile> user@<ipaddr>:/etc/kubernetes/admin.conf $Home/.kube/config
or
pscp -i ~/.kube/<privatekeyfile> user@<ipaddr>:/etc/kubernetes/admin.conf $Home/.kube/config
kuberctl version
```

## Tear down
To undo what kubeadm did, you should first drain the node and make sure that the node is empty before shutting it down.
Talking to the master with the appropriate credentials, run:
```shell
kubectl drain <node name> --delete-local-data --force --ignore-daemonsets
kubectl delete node <node name>
```

Then, on the node being removed, reset all kubeadm installed state:
```shell
kubeadm reset
```

The reset process does not reset or clean up iptables rules or IPVS tables. If you wish to reset iptables, you must do so manually:
```shell
iptables -F && iptables -t nat -F && iptables -t mangle -F && iptables -X
```

If you want to reset the IPVS tables, you must run the following command:
```shell
ipvsadm -C
```
If you wish to start over simply run ```kubeadm init``` or ```kubeadm join``` with the appropriate arguments.

If "kubectl get nodes" shows all of the nodes as one single entry, change each nodes with a different hostname in "/etc/hosts" individually and reinstalling k8s using kubespay, it will show all nodes :-)
